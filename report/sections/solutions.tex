\subsection{Problem 1: The user statistics have to be collected}
On the moment the SQL statement is being constructed to query the database, the aggregation factor and domain are known. At that moment, these parameters are logged for user statistics.
There are two tables which hold information about the queries. The first table: querylog holds information about every parameter the user is able to affect. Each query results in a new row containing the start point, end point and factor used by the graph and a timestamp when the query was executed. The second table: factorlog holds information about the factors used only: when a query contains a new factor which is not documented set in this table, a new row is appended containing the new aggregation factor. Else the count related to the factor in incremented by one.\\

\subsection{Problem 2: Determining the basic aggregation levels}
\subsubsection{Precise method}
To provide a fast response time from the beginning, a set of pre-aggregations is to be calculated based on the expected queries of users. Wombacher and Aly \cite{wombacher2011} showed the Integer Linear Programming (ILP) approach to be the most promising solution to the problem of selecting pre-aggregation factors in a study in which multiple solutions have been reviewed. The Integer Linear Program formulated by Wombacher and Aly \cite{wombacher2011} did not take into account the flexibility of plotting at different resolutions (in our case: 300-600 data points). To be able to use Integer Linear Programming to find an optimal set of pre-aggregation factors we propose a small modification to the Integer Linear Program formulated by Wombacher and Aly \cite{wombacher2011} as follows.
\paragraph{A Integer Linear Program in a multi-resolution setting}
%TODO NIEK: HIER MIJN ILP UITLEGGEN

\paragraph{Choosing the query input}
With no usage statistics available at the beginning it is desirable to optimize the ILP for as many queries as possible. In a study by Meindl and Templ \cite{meindl2012}, the LPsolve solver which Wombacher and Aly \cite{wombacher2011} used to solve the Integer Linear Program performed 19 times slower and was able to find an optimal solution in more than 40 times fewer problems than the best performing ILP-solver included in the study (Gurobi \cite{gurobi}). As the risk of not finding an optimal solution (within feasible time) increases with an increasing number of variables, which is also stated by Wombacher and Aly \cite{wombacher2011}, the use of a more efficient and effective ILP-solver will contribute to our ability of calculating the optimal pre-aggregation factors for larger query sets.\\

A test run showed it to be infeasible to solve the ILP for all queries theoretically possible, as the solver did not terminate within three days time. The question rises which queries are most important to optimize. A query of factor x worst case needs to retrieve x data items (in case no pre-aggregation fits and the non-aggregated data is used). Therefore we can conclude that the larger the query factor, the larger the worst case cost of the query. Our solution is based on factors outputted by the ILP-solver for the 50\% of the theoretically possible queries with the largest factor. 

\subsection{Problem 3: Determining the correct aggregation level to use}
\subsubsection{Fast method}
The standard query simply selects all database entries between two points of time and will aggregate the result set until there are only 600 or fewer (min is 300) points left. Given a set of pre-aggregation tables, we can simply make the standard query use a pre-aggregated dataset instead of the entire database. To determine what pre-aggregation table to use in the query, the following method is executed. Check for all aggregation factors: Is it there an aggregation factor where the following expression is true 0.5 * aggregationFactor < requiredAggregationFactor < aggregationFactor, then the aggregation dataset with this factor should be used, as it will result between 300 - 600 points without requiring run-time aggregation. Otherwise, find largest pre-aggregation factor in the set that is smaller then the required aggregation factor. Using this aggregations dataset will result in the least amount of run-time aggregations to provide between 300-600 aggregations.

%TODO: hoe bepaald de precise method welke pre-aggregated dataset gebruikt word voor de user query?

While this method is not very precise, it does however make sure a minimum of run-time aggregations are done, making the system fast. We believe that for new users, this is a good way to get aquinted with the dataset.

\subsubsection{Precise method}

\subsection{Problem 4: We need a tool to calculate the optimal aggregation factors}
\subsection{Problem 5: The offset problem needs to be addressed}
\subsection{Problem 6: The graphical user interface needs some bugfixing and some extra features}
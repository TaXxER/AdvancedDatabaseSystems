\subsection{Problem 1: The user statistics have to be collected}
On the moment the SQL-statement is being constructed to query the database, the aggregation factor and domain are known. At that moment, these parameters are logged for user statistics.
There are two tables which hold information about the queries. The first table: querylog holds information about every parameter the user is able to affect. Each query results in a new row containing the start point, end point and factor used by the graph and a timestamp when the query was executed. The second table: factorlog holds information about the factors used only: when a query contains a new factor which is not documented set in this table, a new row is appended containing the new aggregation factor. Else, the count related to the factor in incremented by one.\\

\subsection{Problem 2: Determining the basic aggregation levels}
\subsubsection{Precise method}
To provide a fast response time from the beginning, a set of pre-aggregations is to be calculated based on the expected queries of users. Wombacher and Aly \cite{wombacher2011} showed the Integer Linear Programming (ILP) approach to be the most promising solution to the problem of selecting pre-aggregation factors in a study in which multiple solutions have been reviewed. The Integer Linear Program formulated by Wombacher and Aly \cite{wombacher2011} did not take into account the flexibility of plotting at different resolutions (in our case: 300-600 data points). To be able to use Integer Linear Programming to find an optimal set of pre-aggregation factors we propose a small modification to the Integer Linear Program formulated by Wombacher and Aly \cite{wombacher2011} as follows.
\paragraph{A Integer Linear Program in a multi-resolution setting}
To the Integer Linear Program definition given by Wombacher and Aly \cite{wombacher2011}, we redefine the query cost definition $L_{i,j}$ such that it equals the cost for plotting query i using factor j at the resolution resulting in the cheapest cost for this query and factor. The following variable definitions are added or revised:
\begin{itemize}
\item R is the set of allowed resolutions
\item $C_{i,j,r}$ is the cost for executing query $q_{i}$ with pre-aggregation level $A_{j}$ using plotting resolution $R_{r}$; given that the factors are defined on the highest possible resolution, the cost is defined as $\frac{f_{i}*\frac{max(R)}{r}}{f_{i}^a}$ if $f_{i}*\frac{max(R)}{r}$ mod $f_{j}^a = 0$, or a high constant otherwise indicating that the resolution adjusted query factor $f_{i}*\frac{max(R)}{r}$ is not an integer multiple of the pre-aggregate $f_{j}^a$.
\item $L_{i,j}$ is the minimal cost for executing query $q_{i}$ with pre-aggregation level $A_{j}$; the cost is the minimum of the set of costs $C_{i,j,r}$ with $r \in R$.
\end{itemize}

\paragraph{Choosing the query input}
With no usage statistics available at the beginning it is desirable to optimize the ILP for as many queries as possible. In a study by Meindl and Templ \cite{meindl2012}, the LPsolve solver which Wombacher and Aly \cite{wombacher2011} used to solve the Integer Linear Program performed 19 times slower and was able to find an optimal solution in more than 40 times fewer problems than the best performing ILP-solver included in the study (Gurobi \cite{gurobi}). As the risk of not finding an optimal solution (within feasible time) increases with an increasing number of variables, which is also stated by Wombacher and Aly \cite{wombacher2011}, the use of a more efficient and effective ILP-solver will contribute to our ability of calculating the optimal pre-aggregation factors for larger query sets.\\

A test run showed it to be infeasible to solve the ILP for all queries theoretically possible, as the solver did not terminate within three days time. The question rises which queries are most important to optimize. A query of factor x worst case needs to retrieve x data items (in case no pre-aggregation fits and the non-aggregated data is used). Therefore we can conclude that the larger the query factor, the larger the worst case cost of the query. The pre-aggregation factors in precise mode are based on ILP-solver output for the 50\% of the theoretically possible queries with the largest factor.

\subsubsection{No run-time aggregations method}
Combining the facts that we have to show between 300-600 points, the minimum selectable aggregation factor seems to be around 3000 milliseconds and large aggregation factors result in only small pre-aggregated datasets, we can create a set of pre-aggregated datasets so we never have to do any run-time aggregation while staying below 5\% in combined datasets size of the original dataset.

We found the maximum aggregation factor to be around 241884 milliseconds, and if we create a pre-aggregation with this factor, every query between factor 120942 (0.5*241884) and 241884 can be served using this pre-aggregation factor, where in the case of 120942 only 300 points are returned. This dataset however is very small, because it aggregates a lot of data points together. We can once again create a pre-aggregated dataset with factor 12094 to serve every query between 60471 (0.5*12094) and 12094, and can continue to create such pre-aggregated datasets until factor 1890. 

In this way, we can serve every query with pre-aggregated dataset of a factor higher than the required factor, and thus only between 300-600 points are returned, but no run-time aggregation will be required. Because all of these aggregation factors are relatively big, their combined size is relatively only very small.

\subsection{Problem 3: Determining the correct aggregation level to use}
\subsubsection{Fast method}
The standard query simply selects all database entries between two points of time and will aggregate the result set until there are only 600 or fewer (minimum of 300) points left. Given a set of pre-aggregation tables, we can simply make the standard query use a pre-aggregated dataset instead of the entire database. To determine what pre-aggregation table to use in the query, the following method is executed. Check for all aggregation factors: Is it there an aggregation factor where the following expression is true: "0.5 * aggregationFactor < requiredAggregationFactor < aggregationFactor", then the aggregation dataset with this factor should be used, as it will result between 300 - 600 points without requiring run-time aggregation. Otherwise, find largest pre-aggregation factor in the set that is smaller then the required aggregation factor. Using this aggregations dataset will result in the least amount of run-time aggregations to provide between 300-600 aggregations.\\

While this method is not very precise, it does however make sure a minimum of run-time aggregations are done, making the system fast. We believe that for new users, this is a good way to get acquainted with the dataset.

\subsubsection{Precise method}
In the Precise Method section of Problem 2 we redefined the query cost definition $L_{i,j}$ such that it equals the cost for plotting query i using factor j at the resolution resulting in the cheapest cost for this query and factor. We can only benefit from this adjustment to the ILP-problem formulated by Wombacher and Aly \cite{wombacher2011} if we actually use the resolution and factor that resulted in the cheapest cost for a given query at run time. A naive way to do this would be to calculate the cheapest cost factor-resolution combination by looping over the factors as well as over the resolutions and calculate their cost for each query entered. A drawback of this approach is that for each recurring query a calculation needs to be done that was already done earlier. Instead we calculate the least cost factor-resolution combination for each theoretically possible query once at application boot time and keep a map from query to factor and resolution in memory. This map from query to factor and resolution is calculated by looping of all factors and all resolutions for all theoretically possible query factors. Even though this sounds as time-consuming task, this map can be calculated in about a second.

\subsection{Problem 4: We need a tool to calculate the optimal aggregation factors}
The collection of usage statistics enables the possibility of using the implemented ILP-program and the ILP-solver to optimize further on queries that are frequently entered by the user(s). Because optimizing the ILP-program on a large set of queries is a time-consuming job, this should not be done at runtime but could instead be executed as a batch job (e.g. at nigh time). Depending on available computing power, the number of queries that can be used as ILP input such that solver still terminates within reasonable time lies around 50\% of all possible queries. Therefore a selection criterion is needed to choose which queries from the usage statistics table to use as input. A naive way to do this would be to select the 50\% (or so, depending on computing power) of the queries that have the highest count in the usage statistics table. A better way to do this is to take into account some weighting factor biasing the queries with higher query factors, as those queries have a higher worst case cost. One could for example multiply the query counts from the usage statistics table with the query factors and choose the 50\% queries with the highest result on this multiplication. Note however that methods described in the Problem 4 are not part of the implementation.

\subsection{Problem 5: The offset problem needs to be addressed}
There are several possibilities to solve the problem of offsets. The first solution can be to aggregate every factor using every offset. Of course, this solution is simple, but it not very resource efficient. A second possibility is to use a pre-aggregation with the same factor, but with a different offset. Every datapoint in the dataset must then be recomputed by adding a part of another to the average and removing the part of the unwanted domain. For example: if you have pre-aggregated the dataset is 0-11 and your pre-aggregations are 0-2, 3-5, 6-8, 9-11 (factor = 3) then your offset is 0. If you need the aggregation with domain 4-6 with factor = 3. Then you can use the pre-aggregation with the following calculation: add the domain 6-6 and remove domain 3-3. Preferably, you want to use a lower factor pre-aggregation to determine which domains you have to add and remove for each datapoint.

Due to the limited time available for working on the assignment and because the paper\cite{wombacher2011} does not address this problem as well, this problem is not addressed in this project.

\subsection{Problem 6: The graphical user interface needs some bugfixing and some extra features}
From the beginning of the project, we were designated to use the jFreeChart library and the included files to update the graph using the new data points. The task description mentioned already that the GUI still contained some errors and our goal was to fix them.

The first error we came across was that the JdbcYIntervalSeries' update method needed a data conversion to handle the input from the database. The epoch time in the database is defined in seconds, but the epoch time needed in the graph must be defined in milliseconds. To handle this problem, we updated all the methods in JdbcYIntervalSeries by multiplying the time with 1000 and dividing the time variables before the query with 1000.

The second error we came across was that the domain scrollbar left side could surpass his right side. Therefore, the extent became zero and an exception was thrown by jFreeChart. To fix this bug, we updated the adjustmentValueChanged() function in GraphGui class. It now checks if the extent evaluates to zero and updates this variable with a constant factor if so.

Apart from the errors, we also included some extra features in the GUI. The horizontal scrollbar, used for updating the domain of the graph, was not implemented very well. When an user zooms in and reaches (for example) the domain that 600 points would represent a day, the "to-right"-button and the "go-left"-botton would shift the domain with several days. Therefore, navigating to the right or left on this level of detail would be very inconvenient.
To solve this inconvenience, we introduced two buttons who first look at the level of detail and then calculate to which domain the graph should be shifted. If the scrollbar is beginning at point x and the extent of the scrollbar is y, the button "to-right" lets the domain start from x+(y/2) and uses the same extent as before the shifting. Of course, the "to-left" button uses x-(y/2) to determine the start of the domain in the graph.

\subsection{Problem 7: Optimizing the inevitable load operations}
Even with a good pre-aggregated dataset to use, some data will have to be loaded from the database inevitably and this will remain to be the bottleneck of the graph plotting process. Next to limiting the number of rows to load, there are other options to use to decrease the load time even further. 

The simplest example of this idea is to remove the id column from the aggregated datasets, so only the water level and a timestamp remain. Furthermore, the smallest possible data types are used for the values of the pre-aggregated datasets (unsigned int for the timestamp, float for water level). Together this causes each row to be smaller and thus resulting in less data for the same select query and possible less page I/O's.

Another example is the choice to use the MyISAM storage engine instead of the default InnoDB engine for the aggregated datasets as this first one performs better for read operations.

Finally, we also created a B+ Tree index on the timed column to minimize the amount of page loads required to find the page containing the correct rows. As the aggregated datasets are sorted on the values of the timed column, this is a clustered index and thus the search costs to find the correct first page can be estimated to be around 2. 

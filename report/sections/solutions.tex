\subsection{Problem 1: The user statistics have to be collected}
On the moment the SQL-statement is being constructed to query the database, the aggregation factor and domain are known. At that moment, these parameters are logged for user statistics.
There are two tables which hold information about the queries. The first table: query log holds information about every parameter the user is able to affect. Each query results in a new row containing the start point, end point and factor used by the graph and a timestamp when the query was executed. The second table: factor log holds information about the factors used only: when a query contains a new factor which is not documented set in this table, a new row is appended containing the new aggregation factor. Else the count related to the factor in incremented by one.\\

\subsection{Problem 2: Determining the basic aggregation levels}
\subsubsection{Precise method}
To provide a fast response time from the beginning, a set of pre-aggregations is to be calculated based on the expected queries of users. Wombacher and Aly \cite{wombacher2011} showed the Integer Linear Programming (ILP) approach to be the most promising solution to the problem of selecting pre-aggregation factors in a study in which multiple solutions have been reviewed. The Integer Linear Program formulated by Wombacher and Aly \cite{wombacher2011} did not take into account the flexibility of plotting at different resolutions (in our case: 300-600 data points). To be able to use Integer Linear Programming to find an optimal set of pre-aggregation factors we propose a small modification to the Integer Linear Program formulated by Wombacher and Aly \cite{wombacher2011} as follows.
\paragraph{A Integer Linear Program in a multi-resolution setting}
To the Integer Linear Program definition given by Wombacher and Aly \cite{wombacher2011}, we redefine the query cost definition $L_{i,j}$ such that it equals the cost for plotting query i using factor j at the resolution resulting in the cheapest cost for this query and factor. The following variable definitions are added or revised:
\begin{itemize}
\item R is the set of allowed resolutions
\item $C_{i,j,r}$ is the cost for executing query $q_{i}$ with pre-aggregation level $A_{j}$ using plotting resolution $R_{r}$; given that the factors are defined on the highest possible resolution, the cost is defined as $\frac{f_{i}*\frac{max(R)}{r}}{f_{i}^a}$ if $f_{i}*\frac{max(R)}{r}$ mod $f_{j}^a = 0$, or a high constant otherwise indicating that the resolution adjusted query factor $f_{i}*\frac{max(R)}{r}$ is not an integer multiple of the pre-aggregate $f_{j}^a$.
\item $L_{i,j}$ is the minimal cost for executing query $q_{i}$ with pre-aggregation level $A_{j}$; the cost is the minimum of the set of costs $C_{i,j,r}$ with $r \in R$.
\end{itemize}

\paragraph{Choosing the query input}
With no usage statistics available at the beginning it is desirable to optimize the ILP for as many queries as possible. In a study by Meindl and Templ \cite{meindl2012}, the LPsolve solver which Wombacher and Aly \cite{wombacher2011} used to solve the Integer Linear Program performed 19 times slower and was able to find an optimal solution in more than 40 times fewer problems than the best performing ILP-solver included in the study (Gurobi \cite{gurobi}). As the risk of not finding an optimal solution (within feasible time) increases with an increasing number of variables, which is also stated by Wombacher and Aly \cite{wombacher2011}, the use of a more efficient and effective ILP-solver will contribute to our ability of calculating the optimal pre-aggregation factors for larger query sets.\\

A test run showed it to be infeasible to solve the ILP for all queries theoretically possible, as the solver did not terminate within three days time. The question rises which queries are most important to optimize. A query of factor x worst case needs to retrieve x data items (in case no pre-aggregation fits and the non-aggregated data is used). Therefore we can conclude that the larger the query factor, the larger the worst case cost of the query. The pre-aggregation factors in precise mode are based on ILP-solver output for the 50\% of the theoretically possible queries with the largest factor. 

\subsection{Problem 3: Determining the correct aggregation level to use}
\subsubsection{Fast method}
The standard query simply selects all database entries between two points of time and will aggregate the result set until there are only 600 or fewer (minimum of 300) points left. Given a set of pre-aggregation tables, we can simply make the standard query use a pre-aggregated dataset instead of the entire database. To determine what pre-aggregation table to use in the query, the following method is executed. Check for all aggregation factors: Is it there an aggregation factor where the following expression is true 0.5 * aggregationFactor < requiredAggregationFactor < aggregationFactor, then the aggregation dataset with this factor should be used, as it will result between 300 - 600 points without requiring run-time aggregation. Otherwise, find largest pre-aggregation factor in the set that is smaller then the required aggregation factor. Using this aggregations dataset will result in the least amount of run-time aggregations to provide between 300-600 aggregations.\\

While this method is not very precise, it does however make sure a minimum of run-time aggregations are done, making the system fast. We believe that for new users, this is a good way to get acquainted with the dataset.

\subsubsection{Precise method}
In the Precise Method section of Problem 2 we redefined the query cost definition $L_{i,j}$ such that it equals the cost for plotting query i using factor j at the resolution resulting in the cheapest cost for this query and factor. We can only benefit from this adjustment to the ILP-problem formulated by Wombacher and Aly \cite{wombacher2011} if we actually use the resolution and factor that resulted in the cheapest cost for a given query at run time. A naive way to do this would be to calculate the cheapest cost factor-resolution combination by looping over the factors as well as over the resolutions and calculate their cost for each query entered. A drawback of this approach is that for each recurring query a calculation needs to be done that was already done earlier. Instead we calculate the least cost factor-resolution combination for each theoretically possible query once at application boot time and keep a map from query to factor and resolution in memory. This map from query to factor and resolution is calculated by looping of all factors and all resolutions for all theoretically possible query factors. Even though this sounds as time-consuming task, this map can be calculated in about a second.

\subsection{Problem 4: We need a tool to calculate the optimal aggregation factors}
\subsection{Problem 5: The offset problem needs to be addressed}
There are several possibilities to solve the problem of offsets. The first solution can be to aggregate every factor using every offset. Of course, this solution is simple, but it not very recourse efficient. A second possibility is to use a pre-aggregation with the same factor, but with a different offset. Every datapoint in the dataset must then be recomputed by adding a part of another to the average and removing the part of the unwanted domain. For example: if you have pre-aggregated the dataset is 0-11 and your pre-aggregations are 0-2, 3-5, 6-8, 9-11 (factor = 3) then your offset is 0. If you need the aggregation with domain 4-6 with factor = 3. Then you can use the pre-aggregation with the following calculation: add the domain 6-6 and remove 3-3. Preferably, you want to use a lower factor pre-aggregation to determine which domains you have to add and remove for each datapoint.

Due to the limited time available for working on the assignment and because the paper\cite{wombacher2011} does not address this problem as well, this problem is not addressed in this project.

\subsection{Problem 6: The graphical user interface needs some bugfixing and some extra features}
From the beginning of the project, we were designated to use the jFreeChart library and the included files to update the graph using the new data points. The task description mentioned already that the GUI still contained some errors and our goal was to fix them.

The first error we came across was that the JdbcYIntervalSeries' update method needed a data conversion to handle the input from the database. The epoch time in the database is defined in seconds, but the epoch time needed in the graph must be defined in milliseconds. To handle this problem, we updated all the methods in JdbcYIntervalSeries by multiplying the time with 1000 and dividing the time variables before the query with 1000.

The second error we came across was that the domain scrollbar left side could surpass his right side. Therefore, the extent became zero and an exception was thrown by jFreeChart. To fix this bug, we updated the adjustmentValueChanged() function in GraphGui class. It now checks if the extent evaluates to zero and updates this variable with a constant factor if so.

Apart from the errors, we also included some extra features in the GUI. The horizontal scrollbar, used for updating the domain of the graph, was not implemented very well. When an user zooms in and reaches (for example) the domain that 600 points would represent a day, the "to-right"-button and the "go-left"-botton would shift the domain with several days. Therefore, navigating to the right or left on this level of detail would be very inconvenient.
To solve this inconvenience, we introduced two buttons who first look at the level of detail and then calculate to which domain the graph should be shifted. If the scrollbar is beginning at point x and the extent of the scrollbar is y, the button "to-right" lets the domain start from x+(y/2) and uses the same extent as before the shifting. Of course, the "to-left" button uses x-(y/2) to determine the start of the domain in the graph.